{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1408148,"sourceType":"datasetVersion","datasetId":823358},{"sourceId":7658896,"sourceType":"datasetVersion","datasetId":4465590}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Import Libraries and Load Tokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_scheduler\nfrom datasets import load_dataset\nimport pandas as pd\nimport wandb\nfrom torch.cuda.amp import autocast, GradScaler\nimport gc\nfrom tqdm import tqdm\n\n# Initialize WandB in offline mode\nwandb.init(project=\"DeBERTa-Fake-News\", config={\"epochs\": 3, \"batch_size\": 32, \"lr\": 1e-5}, mode=\"offline\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\n# Free GPU cache\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Load DeBERTa tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:31:30.542654Z","iopub.execute_input":"2025-03-29T15:31:30.542870Z","iopub.status.idle":"2025-03-29T15:31:58.750277Z","shell.execute_reply.started":"2025-03-29T15:31:30.542848Z","shell.execute_reply":"2025-03-29T15:31:58.749548Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4a75b93ac74b1caac5753498997c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40ab28b8699b4101bffc8cbeca367d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85693a71ee06437cbb2f869927db4a94"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load Datasets\n# Load dataset from Hugging Face for pre-training\nds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\", split=\"train\")\npretrain_texts = ds[\"text\"][:int(0.05 * len(ds[\"text\"]))]  # Use 10% of Wikitext-103\npretrain_df = pd.DataFrame({\"text\": pretrain_texts, \"label\": [0] * len(pretrain_texts)})\n\n# Load dataset from CSV (WELFake) for fine-tuning\nwelfake_df = pd.read_csv(\"/kaggle/input/welfake-dataset-for-fake-news/WELFake_Dataset.csv\", usecols=[\"text\", \"label\"], dtype={\"label\": str})\n\n# Ensure labels are numeric & clean\nwelfake_df[\"label\"] = welfake_df[\"label\"].str.replace(r\"[^\\d]\", \"\", regex=True).str.strip()\nwelfake_df = welfake_df[welfake_df[\"label\"] != \"\"]  # Remove empty values\nwelfake_df[\"label\"] = welfake_df[\"label\"].astype(int)  # Convert to int for PyTorch\nprint(\"Data loading complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:32:01.367312Z","iopub.execute_input":"2025-03-29T15:32:01.367734Z","iopub.status.idle":"2025-03-29T15:32:18.081242Z","shell.execute_reply.started":"2025-03-29T15:32:01.367698Z","shell.execute_reply":"2025-03-29T15:32:18.080238Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0109c52f0424918a4e6bf3e765fe3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b82310686704efaa359c328e89e8d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10fe7176e15423884eb4b22fd1a1fcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e68f1a15c14569acb883b7521adeec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53efb1cb99647d098866b581604d41f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aaa6997ffa43cca7876eb3237f06f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749109ea80704bf983b5463377ea7b3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3396bb744ecf4a308babfe65f54183bc"}},"metadata":{}},{"name":"stdout","text":"Data loading complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Define Dataset Classes\nclass MLMDataset(Dataset):\n    def __init__(self, texts, max_len=128):\n        self.encodings = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")\n        self.labels = self.encodings.input_ids.clone()\n        rand = torch.rand(self.labels.shape)\n        mask_arr = (rand < 0.15) * (self.labels != tokenizer.pad_token_id) * (self.labels != tokenizer.cls_token_id) * (self.labels != tokenizer.sep_token_id)\n        self.encodings.input_ids[mask_arr] = tokenizer.mask_token_id\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items()}, self.labels[idx]\n\nclass FakeNewsDataset(Dataset):\n    def __init__(self, df, max_len=256):\n        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n        self.encodings = tokenizer(df[\"text\"].astype(str).tolist(), padding=\"max_length\", truncation=True, \n                                   max_length=max_len, return_tensors=\"pt\")\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items() if key != \"token_type_ids\"}, self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:32:24.770560Z","iopub.execute_input":"2025-03-29T15:32:24.770904Z","iopub.status.idle":"2025-03-29T15:32:24.783559Z","shell.execute_reply.started":"2025-03-29T15:32:24.770874Z","shell.execute_reply":"2025-03-29T15:32:24.782721Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Define DeBERTa Model\nclass DeBERTaClassifier(nn.Module):\n    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=2):\n        super(DeBERTaClassifier, self).__init__()\n        self.deberta = AutoModel.from_pretrained(model_name)\n        self.deberta.gradient_checkpointing_enable()\n        self.classifier = nn.Linear(self.deberta.config.hidden_size, num_labels)\n        self.mlm_head = nn.Linear(self.deberta.config.hidden_size, self.deberta.config.vocab_size)  # MLM head for pretraining\n    \n    def forward(self, input_ids, attention_mask, mlm=False):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        if mlm:\n            return self.mlm_head(outputs.last_hidden_state)\n        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:32:27.511409Z","iopub.execute_input":"2025-03-29T15:32:27.511836Z","iopub.status.idle":"2025-03-29T15:32:27.520769Z","shell.execute_reply.started":"2025-03-29T15:32:27.511803Z","shell.execute_reply":"2025-03-29T15:32:27.519735Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cell 5: Initialize Model, Optimizer, and Scheduler\nmodel = DeBERTaClassifier().to(device)\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n# Learning rate scheduler\nnum_training_steps = 3 * len(welfake_df) // 32  # Adjusted for batch size 32\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\nscaler = GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:32:30.477597Z","iopub.execute_input":"2025-03-29T15:32:30.478021Z","iopub.status.idle":"2025-03-29T15:32:48.350134Z","shell.execute_reply.started":"2025-03-29T15:32:30.477984Z","shell.execute_reply":"2025-03-29T15:32:48.348959Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea13f458bcb54c3e9058690448bada86"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n<ipython-input-5-17bdeab1d406>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 6: Pre-training Loop\ndef pretrain_model(model, train_loader, epochs=1):\n    model.train()\n    mlm_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    for epoch in range(epochs):\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Pretraining Epoch {epoch+1}\")\n        for batch in progress_bar:\n            inputs, labels = batch\n            inputs = {key: val.to(device) for key, val in inputs.items()}\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], mlm=True)\n                loss = mlm_criterion(outputs.view(-1, model.deberta.config.vocab_size), labels.view(-1))\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n            progress_bar.set_postfix(loss=total_loss / len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:32:58.187434Z","iopub.execute_input":"2025-03-29T15:32:58.188545Z","iopub.status.idle":"2025-03-29T15:32:58.195920Z","shell.execute_reply.started":"2025-03-29T15:32:58.188514Z","shell.execute_reply":"2025-03-29T15:32:58.195027Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 7: Run Pre-training\npretrain_dataset = MLMDataset(pretrain_texts)\npretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\npretrain_model(model, pretrain_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:33:00.559828Z","iopub.execute_input":"2025-03-29T15:33:00.560223Z","iopub.status.idle":"2025-03-29T17:50:44.221821Z","shell.execute_reply.started":"2025-03-29T15:33:00.560174Z","shell.execute_reply":"2025-03-29T17:50:44.220716Z"}},"outputs":[{"name":"stderr","text":"Pretraining Epoch 1: 100%|██████████| 2815/2815 [2:17:26<00:00,  2.93s/it, loss=2.26]  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 8: Fine-tuning Loop\n# Update Dataset with Reduced Sequence Length\nclass FakeNewsDataset(Dataset):\n    def __init__(self, df, max_len=128):  # Reduced max_len from 256 to 128\n        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n        self.encodings = tokenizer(df[\"text\"].astype(str).tolist(), \n                                   padding=\"max_length\", truncation=True, \n                                   max_length=max_len, return_tensors=\"pt\")\n    \n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items() if key != \"token_type_ids\"}, self.labels[idx]\n\n# Reload Training Data with New Dataset Class\ntrain_dataset = FakeNewsDataset(welfake_df)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Update Learning Rate Scheduler with Fewer Warmup Steps\nnum_training_steps = len(train_loader) * 1  # Only 1 epoch\nnum_warmup_steps = int(0.02 * num_training_steps)  # 2% Warmup\n\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, \n                             num_warmup_steps=num_warmup_steps, \n                             num_training_steps=num_training_steps)\n\n# Updated Fine-Tuning Loop (1 Epoch)\ndef fine_tune_model(model, train_loader):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    progress_bar = tqdm(train_loader, desc=\"Fine-tuning Epoch 1\")\n    \n    for batch in progress_bar:\n        inputs, labels = batch\n        inputs = {key: val.to(device) for key, val in inputs.items()}\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        with torch.amp.autocast(\"cuda\"):\n            outputs = model(**inputs)\n            loss = criterion(outputs, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        lr_scheduler.step()\n        \n        # Compute Accuracy\n        preds = torch.argmax(outputs, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix(loss=total_loss / (total + 1e-8), accuracy=correct / total)\n\n# Run Fine-Tuning (1 Epoch)\nfine_tune_model(model, train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:05:54.865742Z","iopub.execute_input":"2025-03-29T18:05:54.866070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Run Fine-tuning\ntrain_dataset = FakeNewsDataset(welfake_df)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntrain_model(model, train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:57:59.868934Z","iopub.execute_input":"2025-03-29T17:57:59.869268Z","iopub.status.idle":"2025-03-29T18:05:44.830861Z","shell.execute_reply.started":"2025-03-29T17:57:59.869242Z","shell.execute_reply":"2025-03-29T18:05:44.829674Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n<ipython-input-16-1c87c8f286f8>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nFine-tuning Epoch 1:   0%|          | 0/2255 [00:00<?, ?it/s]<ipython-input-16-1c87c8f286f8>:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nFine-tuning Epoch 1:  17%|█▋        | 376/2255 [06:25<32:04,  1.02s/it, accuracy=0.928, loss=0.029] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-a154faa1a1cd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFakeNewsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwelfake_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-1c87c8f286f8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# Cell 10: Save and Evaluate Model\ntorch.save(model.state_dict(), \"deberta_fakenews.pth\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Install Required Packages\n!pip install rouge-score nltk scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Model Evaluation and Testing\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, log_loss, matthews_corrcoef\nimport numpy as np\nimport torch.nn.functional as F\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge import Rouge\nimport torch\n\ndef evaluate_model(model, test_loader, task=\"classification\"):\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    total_loss = 0\n    criterion = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating Model\"):\n            inputs, labels = batch\n            inputs = {key: val.to(device) for key, val in inputs.items()}\n            labels = labels.to(device)\n            \n            if task == \"mlm\":\n                outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], mlm=True)\n                loss = criterion(outputs.view(-1, model.deberta.config.vocab_size), labels.view(-1))\n                total_loss += loss.item()\n            else:\n                logits = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n                probs = F.softmax(logits, dim=1)\n                preds = torch.argmax(probs, dim=1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_probs.extend(probs[:, 1].cpu().numpy())\n\n    if task == \"mlm\":\n        perplexity = np.exp(total_loss / len(test_loader))\n        print(f\"MLM Perplexity: {perplexity:.4f}\")\n        return perplexity\n\n    # Compute classification metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, zero_division=0)\n    recall = recall_score(all_labels, all_preds, zero_division=0)\n    f1 = f1_score(all_labels, all_preds, zero_division=0)\n    roc_auc = roc_auc_score(all_labels, all_probs)\n    logloss = log_loss(all_labels, all_probs)\n    mcc = matthews_corrcoef(all_labels, all_preds)\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n    print(\"\\nConfusion Matrix:\\n\", cm)\n    print(f\"\\nMetrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1:.4f}, AUC-ROC={roc_auc:.4f}, Log Loss={logloss:.4f}, MCC={mcc:.4f}\")\n\n# Evaluate Fine-Tuned Model\nevaluate_model(model, train_loader, task=\"classification\")\n\n# Evaluate Pretrained MLM Model\nevaluate_model(model, pretrain_loader, task=\"mlm\")\n\n# Compute ROUGE and BLEU for MLM (Language Modeling)\nrouge = Rouge()\nbleu_scores = []\nrouge_scores = {\"rouge-1\": [], \"rouge-2\": [], \"rouge-l\": []}\n\nfor i in range(10):  # Evaluate on 10 random sentences\n    text = pretrain_texts[i]\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], mlm=True)\n    \n    predicted_tokens = torch.argmax(outputs, dim=-1)[0].cpu().numpy()\n    predicted_text = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n    \n    reference = text.split()\n    candidate = predicted_text.split()\n    \n    bleu = sentence_bleu([reference], candidate)\n    bleu_scores.append(bleu)\n    \n    rouge_score = rouge.get_scores(predicted_text, text)[0]\n    for key in rouge_scores.keys():\n        rouge_scores[key].append(rouge_score[key][\"f\"])\n\n# Output BLEU & ROUGE Scores\nprint(f\"\\nBLEU Score: {np.mean(bleu_scores):.4f}\")\nprint(f\"ROUGE-1: {np.mean(rouge_scores['rouge-1']):.4f}, ROUGE-2: {np.mean(rouge_scores['rouge-2']):.4f}, ROUGE-L: {np.mean(rouge_scores['rouge-l']):.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}